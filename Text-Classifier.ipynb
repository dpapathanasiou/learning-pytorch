{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier\n",
    "\n",
    "Chapter 5 of Programming PyTorch for Deep Learning, but using samples from the [TREC 2005 Spam Corpus](https://trec.nist.gov/data/spam.html) instead of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data \n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenize(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=my_tokenize)\n",
    "LABEL = data.Field(lower=True)\n",
    "samples = data.TabularDataset(path='./data/ham-spam-samples.tsv',\n",
    "                              format='tsv', \n",
    "                              fields=[(\"label\",LABEL), (\"statement\",TEXT)],\n",
    "                              skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181, 60, 60)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training, testing, validating) = samples.split(split_ratio=[0.6,0.2,0.2])\n",
    "(len(training),len(testing),len(validating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 289),\n",
       " ('to', 224),\n",
       " ('for', 140),\n",
       " ('and', 136),\n",
       " ('a', 114),\n",
       " ('of', 109),\n",
       " ('you', 106),\n",
       " ('in', 98),\n",
       " ('is', 95),\n",
       " ('your', 81)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 60\n",
    "TEXT.build_vocab(training, max_size = vocab_size)\n",
    "LABEL.build_vocab(training)\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ['ham'],\n",
       " 'statement': ['per',\n",
       "  'our',\n",
       "  'discussion',\n",
       "  'at',\n",
       "  'the',\n",
       "  'controls',\n",
       "  'meeting',\n",
       "  '-',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'you',\n",
       "  'might',\n",
       "  'be',\n",
       "  'interested',\n",
       "  'in',\n",
       "  'ken',\n",
       "  \"cessac's\",\n",
       "  'recent',\n",
       "  'findings',\n",
       "  'on',\n",
       "  'how',\n",
       "  'the',\n",
       "  'field',\n",
       "  'could',\n",
       "  'be',\n",
       "  'more',\n",
       "  'proactive',\n",
       "  'at',\n",
       "  'finding',\n",
       "  'and',\n",
       "  'correcting',\n",
       "  'meter',\n",
       "  'errors.',\n",
       "  '-----original',\n",
       "  'message-----',\n",
       "  'from:',\n",
       "  'cessac,',\n",
       "  'kenneth',\n",
       "  'sent:',\n",
       "  'tuesday,',\n",
       "  'july',\n",
       "  '03,',\n",
       "  '2001',\n",
       "  '6:15',\n",
       "  'pm',\n",
       "  'to:',\n",
       "  'datta-barua,',\n",
       "  'lohit;',\n",
       "  'alters,',\n",
       "  'dennis;',\n",
       "  'anderson,',\n",
       "  'gary',\n",
       "  'e.;',\n",
       "  'armstrong,',\n",
       "  'julie;',\n",
       "  'asante,',\n",
       "  'ben;',\n",
       "  'bellard,',\n",
       "  'dannis;',\n",
       "  'blair,',\n",
       "  'lynn;',\n",
       "  'br']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(training.examples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "(training, validating, testing), \n",
    "batch_size = 32,\n",
    "device = device,\n",
    "sort_key = lambda x: len(x.statement),\n",
    "sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "\n",
    "Start with a simple [Long short-term memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1)\n",
    "        self.predictor = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        output, (hidden, _) = self.encoder(self.embedding(seq))\n",
    "        preds = self.predictor(hidden.squeeze(0))\n",
    "        return preds.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "    m = nn.Sigmoid()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(batch.statement)\n",
    "            loss = criterion(m(predict), batch.label.double())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * batch.statement.size(0)\n",
    "        training_loss /= len(train_iterator)\n",
    " \n",
    "        model.eval()\n",
    "        for batch_idx,batch in enumerate(valid_iterator):\n",
    "            predict = model(batch.statement)\n",
    "            loss = criterion(m(predict), batch.label.double())\n",
    "            valid_loss += loss.data.item() * batch.statement.size(0)\n",
    " \n",
    "        valid_loss /= len(valid_iterator)\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}'.format(epoch, training_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicLSTM(\n",
       "  (embedding): Embedding(62, 300)\n",
       "  (encoder): LSTM(300, 100)\n",
       "  (predictor): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BasicLSTM(100, 300, 62)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/denis/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 21])) that is different to the input size (torch.Size([21, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/denis/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 28])) that is different to the input size (torch.Size([28, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: -483.59, Validation Loss: -766.79\n",
      "Epoch: 2, Training Loss: -1178.51, Validation Loss: -1314.51\n",
      "Epoch: 3, Training Loss: -1836.56, Validation Loss: -1869.30\n",
      "Epoch: 4, Training Loss: -2538.61, Validation Loss: -2424.09\n",
      "Epoch: 5, Training Loss: -3062.35, Validation Loss: -2867.16\n"
     ]
    }
   ],
   "source": [
    "train(5, model, optimizer, criterion, train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text):\n",
    "    categories = {0: 'ham', 1: 'spam'}\n",
    "    processed = TEXT.process([TEXT.preprocess(text)])\n",
    "    processed = processed.to(device)\n",
    "    return categories[model(processed).argmax().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_text(testing.examples[0].statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.examples[0].label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct   --> 1/1 right overall\n",
      "Correct   --> 2/2 right overall\n",
      "Correct   --> 3/3 right overall\n",
      "Incorrect --> 3/4 right overall\n",
      "Correct   --> 4/5 right overall\n",
      "Correct   --> 5/6 right overall\n",
      "Correct   --> 6/7 right overall\n",
      "Incorrect --> 6/8 right overall\n",
      "Correct   --> 7/9 right overall\n",
      "Correct   --> 8/10 right overall\n",
      "Correct   --> 9/11 right overall\n",
      "Incorrect --> 9/12 right overall\n",
      "Incorrect --> 9/13 right overall\n",
      "Incorrect --> 9/14 right overall\n",
      "Correct   --> 10/15 right overall\n",
      "Correct   --> 11/16 right overall\n",
      "Incorrect --> 11/17 right overall\n",
      "Incorrect --> 11/18 right overall\n",
      "Correct   --> 12/19 right overall\n",
      "Correct   --> 13/20 right overall\n",
      "Correct   --> 14/21 right overall\n",
      "Incorrect --> 14/22 right overall\n",
      "Correct   --> 15/23 right overall\n",
      "Incorrect --> 15/24 right overall\n",
      "Correct   --> 16/25 right overall\n",
      "Correct   --> 17/26 right overall\n",
      "Correct   --> 18/27 right overall\n",
      "Incorrect --> 18/28 right overall\n",
      "Correct   --> 19/29 right overall\n",
      "Correct   --> 20/30 right overall\n",
      "Correct   --> 21/31 right overall\n",
      "Incorrect --> 21/32 right overall\n",
      "Correct   --> 22/33 right overall\n",
      "Correct   --> 23/34 right overall\n",
      "Correct   --> 24/35 right overall\n",
      "Incorrect --> 24/36 right overall\n",
      "Correct   --> 25/37 right overall\n",
      "Correct   --> 26/38 right overall\n",
      "Incorrect --> 26/39 right overall\n",
      "Correct   --> 27/40 right overall\n",
      "Incorrect --> 27/41 right overall\n",
      "Incorrect --> 27/42 right overall\n",
      "Correct   --> 28/43 right overall\n",
      "Correct   --> 29/44 right overall\n",
      "Incorrect --> 29/45 right overall\n",
      "Correct   --> 30/46 right overall\n",
      "Incorrect --> 30/47 right overall\n",
      "Incorrect --> 30/48 right overall\n",
      "Correct   --> 31/49 right overall\n",
      "Correct   --> 32/50 right overall\n",
      "Correct   --> 33/51 right overall\n",
      "Correct   --> 34/52 right overall\n",
      "Incorrect --> 34/53 right overall\n",
      "Correct   --> 35/54 right overall\n",
      "Correct   --> 36/55 right overall\n",
      "Correct   --> 37/56 right overall\n",
      "Correct   --> 38/57 right overall\n",
      "Correct   --> 39/58 right overall\n",
      "Correct   --> 40/59 right overall\n",
      "Correct   --> 41/60 right overall\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "examined = 0\n",
    "for test_example in testing.examples:\n",
    "    actual = test_example.label[0]\n",
    "    predicted = classify_text(test_example.statement)\n",
    "    examined += 1\n",
    "    if actual == predicted:\n",
    "        correct += 1\n",
    "        print('Correct   --> {}/{} right overall'.format(correct, examined))\n",
    "    else:\n",
    "        print('Incorrect --> {}/{} right overall'.format(correct, examined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
