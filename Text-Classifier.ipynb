{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classifier\n",
    "\n",
    "Chapter 5 of Programming PyTorch for Deep Learning, but using samples from the [TREC 2005 Spam Corpus](https://trec.nist.gov/data/spam.html) instead of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data \n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenize(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=my_tokenize)\n",
    "LABEL = data.Field(lower=True)\n",
    "samples = data.TabularDataset(path='./data/ham-spam-samples.tsv',\n",
    "                              format='tsv', \n",
    "                              fields=[(\"label\",LABEL), (\"statement\",TEXT)],\n",
    "                              skip_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 80, 80)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training, testing, validating) = samples.split(split_ratio=[0.6,0.2,0.2])\n",
    "(len(training),len(testing),len(validating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2001', 500),\n",
       " ('jul', 464),\n",
       " ('by', 326),\n",
       " ('with', 297),\n",
       " ('for', 271),\n",
       " ('from:', 270),\n",
       " ('to:', 260),\n",
       " ('id', 249),\n",
       " ('5', 200),\n",
       " ('from', 197)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 80\n",
    "TEXT.build_vocab(training, max_size = vocab_size)\n",
    "LABEL.build_vocab(training)\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ['spam'],\n",
       " 'statement': ['y',\n",
       "  'mailman.enron.com',\n",
       "  '(8.10.1/8.10.1/corp-1.06)',\n",
       "  'with',\n",
       "  'esmtp',\n",
       "  'id',\n",
       "  'g343bbl51389',\n",
       "  'for',\n",
       "  '<matt.motley@enron.com>;',\n",
       "  'wed,',\n",
       "  '4',\n",
       "  'jul',\n",
       "  '2001',\n",
       "  '14:51:50',\n",
       "  '-0500',\n",
       "  '(cdt)',\n",
       "  'date:',\n",
       "  'wed,',\n",
       "  '4',\n",
       "  'jul',\n",
       "  '2001',\n",
       "  '19:48:22',\n",
       "  '+0000',\n",
       "  'from:',\n",
       "  '=?windows-1251?b?0oxq6+ds7e7lio/w5etr7ubl7ejl?=',\n",
       "  '<dongming@aguascalientes.com',\n",
       "  '>',\n",
       "  'x-mailer:',\n",
       "  'the',\n",
       "  'bat!',\n",
       "  '(v2.01)',\n",
       "  'reply-to:',\n",
       "  '=?windows-1251?b?0oxq6+ds7e7lio/w5etr7ubl7ejl?=',\n",
       "  '<dongming@aguascalientes.com',\n",
       "  '>',\n",
       "  'x-priority:',\n",
       "  '3',\n",
       "  '(normal)',\n",
       "  'message-id:',\n",
       "  '<183920372.20041230020824@>',\n",
       "  'to:',\n",
       "  'matt.motley@enron.com',\n",
       "  'subject:',\n",
       "  '=?windows-1251?b?0oxq6+ds4dogwmvkzspoy9wgyidnzsloznmgw87e0yehisddy8jszdvfimlizcag?=',\n",
       "  '=?windows-1251?b?ycdkzs3c38riiseh?=',\n",
       "  'mime-version:',\n",
       "  '1.0',\n",
       "  'content-type:',\n",
       "  'text/plain;',\n",
       "  'charset=windows-1251',\n",
       "  'content-transfer-encoding:',\n",
       "  '8bit',\n",
       "  '��������',\n",
       "  '�',\n",
       "  '������',\n",
       "  '����!!!',\n",
       "  '�������',\n",
       "  '����',\n",
       "  '�',\n",
       "  '�������!!!',\n",
       "  '���������',\n",
       "  '����',\n",
       "  '�',\n",
       "  '�������!',\n",
       "  '�',\n",
       "  '���������']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(training.examples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "(training, validating, testing), \n",
    "batch_size = 32,\n",
    "device = device,\n",
    "sort_key = lambda x: len(x.statement),\n",
    "sort_within_batch = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "\n",
    "Start with a simple [Long short-term memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_dim, vocab_size):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=1)\n",
    "        self.predictor = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        output, (hidden, _) = self.encoder(self.embedding(seq))\n",
    "        preds = self.predictor(hidden.squeeze(0))\n",
    "        return preds.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):\n",
    "    m = nn.Sigmoid()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_iterator):\n",
    "            optimizer.zero_grad()\n",
    "            predict = model(batch.statement)\n",
    "            loss = criterion(m(predict), batch.label.double())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * batch.statement.size(0)\n",
    "        training_loss /= len(train_iterator)\n",
    " \n",
    "        model.eval()\n",
    "        for batch_idx,batch in enumerate(valid_iterator):\n",
    "            predict = model(batch.statement)\n",
    "            loss = criterion(m(predict), batch.label.double())\n",
    "            valid_loss += loss.data.item() * batch.statement.size(0)\n",
    " \n",
    "        valid_loss /= len(valid_iterator)\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}'.format(epoch, training_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicLSTM(\n",
       "  (embedding): Embedding(82, 300)\n",
       "  (encoder): LSTM(300, 100)\n",
       "  (predictor): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BasicLSTM(100, 300, 82)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-2)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "/home/denis/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([1, 16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: -848.09, Validation Loss: -1355.50\n",
      "Epoch: 2, Training Loss: -2253.47, Validation Loss: -2485.72\n",
      "Epoch: 3, Training Loss: -3649.51, Validation Loss: -3622.20\n",
      "Epoch: 4, Training Loss: -4975.94, Validation Loss: -4550.91\n",
      "Epoch: 5, Training Loss: -5842.60, Validation Loss: -4923.16\n"
     ]
    }
   ],
   "source": [
    "train(5, model, optimizer, criterion, train_iterator, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
