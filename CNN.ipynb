{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Chapter 3 of [Programming PyTorch for Deep Learning](https://www.oreilly.com/library/view/programming-pytorch-for/9781492045342/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We can just reuse the same optimizer and training function from the [previous notebook](./Simple-Image-Classifier.ipynb) with some minor tweaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU()\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "cnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cnn.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, training_loader, validation_loader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        model.train()\n",
    "        for (inputs, targets) in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(training_loader.dataset)\n",
    "        \n",
    "        number_correct = 0\n",
    "        number_examples = 0\n",
    "        model.eval()\n",
    "        for (inputs, targets) in validation_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            validation_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(outputs, dim=1), dim=1)[1], targets).view(-1)\n",
    "            number_correct += torch.sum(correct).item()\n",
    "            number_examples += correct.shape[0]\n",
    "        validation_loss /= len(validation_loader.dataset)\n",
    "        \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(\n",
    "            epoch, \n",
    "            training_loss,\n",
    "            validation_loss, \n",
    "            number_correct / number_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Use the same data from the [previous notebook](./Simple-Image-Classifier.ipynb) and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose(\n",
    "    [transforms.Resize((64,64)), \n",
    "     transforms.ToTensor(),    \n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],                    \n",
    "                          std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data   = torchvision.datasets.ImageFolder(root=\"./data/train\",    transform=image_transform)\n",
    "validation_data = torchvision.datasets.ImageFolder(root=\"./data/validate\", transform=image_transform)\n",
    "test_data       = torchvision.datasets.ImageFolder(root=\"./data/test\",     transform=image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "training_loader   = DataLoader(training_data, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size)\n",
    "test_loader       = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 2.14, Validation Loss: 0.71, accuracy = 0.50\n",
      "Epoch: 1, Training Loss: 0.66, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 2, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 3, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 4, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 5, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 6, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 7, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 8, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 9, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.50\n",
      "Epoch: 10, Training Loss: 0.69, Validation Loss: 0.69, accuracy = 0.90\n",
      "Epoch: 11, Training Loss: 0.69, Validation Loss: 0.67, accuracy = 0.80\n",
      "Epoch: 12, Training Loss: 0.67, Validation Loss: 0.64, accuracy = 0.75\n",
      "Epoch: 13, Training Loss: 0.63, Validation Loss: 0.55, accuracy = 0.80\n",
      "Epoch: 14, Training Loss: 0.53, Validation Loss: 0.45, accuracy = 0.90\n",
      "Epoch: 15, Training Loss: 0.45, Validation Loss: 0.37, accuracy = 0.90\n",
      "Epoch: 16, Training Loss: 0.27, Validation Loss: 0.40, accuracy = 0.90\n",
      "Epoch: 17, Training Loss: 0.28, Validation Loss: 1.00, accuracy = 0.80\n",
      "Epoch: 18, Training Loss: 0.37, Validation Loss: 0.38, accuracy = 0.82\n",
      "Epoch: 19, Training Loss: 0.18, Validation Loss: 0.31, accuracy = 0.95\n"
     ]
    }
   ],
   "source": [
    "train(cnn, optimizer, nn.CrossEntropyLoss(), training_loader, validation_loader, 20, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it better than the simpler classifier in the previous notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    i = 0\n",
    "    for (inputs, targets) in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs, dim=1), dim=1)[1], targets)\n",
    "        right = len(list(filter(lambda x: x.item(), list(correct))))\n",
    "        wrong = batch_size - right\n",
    "        print('Batch {}: {} right vs {} wrong (pct correct {:.2f})'.format(i, right, wrong, (right / batch_size)))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: 7 right vs 3 wrong (pct correct 0.70)\n",
      "Batch 1: 9 right vs 1 wrong (pct correct 0.90)\n",
      "Batch 2: 5 right vs 5 wrong (pct correct 0.50)\n",
      "Batch 3: 6 right vs 4 wrong (pct correct 0.60)\n"
     ]
    }
   ],
   "source": [
    "test(cnn, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum: discovering the optimal learning rate\n",
    "\n",
    "Chapter 4 covers `Transfer Learning` and explains a technique for finding the optimal learning rate based on [this paper](https://arxiv.org/abs/1506.01186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def find_lr(model, optimizer, loss_fn, train_loader, device, init_value=1e-8, final_value=10.0):\n",
    "    number_in_epoch = len(train_loader) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for data in train_loader:\n",
    "        batch_num += 1\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Crash out if loss explodes\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            if(len(log_lrs) > 20):\n",
    "                return log_lrs[10:-5], losses[10:-5]\n",
    "            else:\n",
    "                return log_lrs, losses\n",
    "\n",
    "        # Record the best loss\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the values\n",
    "        losses.append(loss.item())\n",
    "        log_lrs.append((lr))\n",
    "\n",
    "        # Do the backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the lr for the next step and store\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "        \n",
    "    if(len(log_lrs) > 20):\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "    else:\n",
    "        return log_lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs, losses = find_lr(cnn, optimizer, nn.CrossEntropyLoss(), training_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEKCAYAAADAVygjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASm0lEQVR4nO3df4xlZX3H8fcHVrCgLL8WRZbtUkHN2hobb1GjNrQCQlJdVGLAmi6CISYlRomJGNuiqBGNFVNR242/qK0iosZVq4gK0eKvnUWlrohsEcsIlaVL0a1WBL794x7sON7ZuXNnnrkzO+9XcjPnec5z7vOdhzv74dxzf6SqkCSphX3GXYAkae9lyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKmZVeMuYDEdfvjhtX79+nGXIUnLyrZt2+6qqjWjHLuiQmb9+vVMTEyMuwxJWlaS/GjUY326TJLUjCEjSWrGkJEkNWPISJKaMWQkSc0YMpKkZgwZSVIzhowkqRlDRpLUjCEjSWrGkJEkNWPISJKaMWQkSc0YMpKkZgwZSVIzhowkqRlDRpLUjCEjSWrGkJEkNWPISJKaMWQkSc0YMpKkZgwZSVIzhowkqZmxhkySU5LclGRHkgsG7N8/yUe6/d9Isn7a/nVJdid55WLVLEka3thCJsm+wDuBU4ENwJlJNkwbdg5wd1UdC1wCvHna/kuAz7auVZI0mnGeyRwP7KiqW6rqXuByYOO0MRuBy7rtK4FnJglAktOAW4Dti1SvJGmOxhkyRwG3TWlPdn0Dx1TVfcA9wGFJDgReBbxutkmSnJtkIsnEzp07F6RwSdJwxhkyGdBXQ455HXBJVe2ebZKq2lxVvarqrVmzZoQyJUmjWjXGuSeBo6e01wK3zzBmMskqYDWwC3gycHqStwAHAw8k+d+qurR92ZKkYY0zZLYCxyU5BvgxcAbwwmljtgCbgK8BpwNfqqoCnvHggCSvBXYbMJK09IwtZKrqviTnAVcB+wLvq6rtSS4CJqpqC/Be4INJdtA/gzljXPVKkuYu/RODlaHX69XExMS4y5CkZSXJtqrqjXKs7/iXJDVjyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDVjyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDVjyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDVjyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKmZsYZMklOS3JRkR5ILBuzfP8lHuv3fSLK+6z8pybYk/9b9/NPFrl2SNLuxhUySfYF3AqcCG4Azk2yYNuwc4O6qOha4BHhz138X8Oyq+gNgE/DBxalakjQX4zyTOR7YUVW3VNW9wOXAxmljNgKXddtXAs9Mkqr6VlXd3vVvBx6aZP9FqVqSNLRxhsxRwG1T2pNd38AxVXUfcA9w2LQxzwe+VVW/bFSnJGlEq8Y4dwb01VzGJHk8/afQTp5xkuRc4FyAdevWzb1KSdLIxnkmMwkcPaW9Frh9pjFJVgGrgV1dey3wCeAvqurfZ5qkqjZXVa+qemvWrFnA8iVJsxlnyGwFjktyTJL9gDOALdPGbKF/YR/gdOBLVVVJDgY+A7y6qq5btIolSXMytpDprrGcB1wF3AhcUVXbk1yU5DndsPcChyXZAZwPPPgy5/OAY4G/TvLt7nbEIv8KkqRZpGr6ZZC9V6/Xq4mJiXGXIUnLSpJtVdUb5Vjf8S9JasaQkSQ1Y8hIkpoxZCRJzRgykqRmDBlJUjOGjCSpGUNGktSMISNJasaQkSQ1Y8hIkpoxZCRJzRgykqRmDBlJUjOGjCSpGUNGktSMISNJasaQkSQ1Y8hIkpoxZCRJzRgykqRmDBlJUjOGjCSpGUNGktSMISNJasaQkSQ1M1TIJHl0kv277ROSvCzJwW1LkyQtd8OeyXwMuD/JscB7gWOADzWrSpK0Vxg2ZB6oqvuA5wJvr6pXAEe2K0uStDcYNmR+leRMYBPw6a7vIW1KkiTtLYYNmRcDTwXeWFU/THIM8E/typIk7Q2GCpmq+l5VvayqPpzkEODhVXXxfCdPckqSm5LsSHLBgP37J/lIt/8bSdZP2ffqrv+mJM+aby2SpIU37KvLrk1yUJJDge8A70/ytvlMnGRf4J3AqcAG4MwkG6YNOwe4u6qOBS4B3twduwE4A3g8cArwru7+JElLyKohx62uqp8meQnw/qq6MMkN85z7eGBHVd0CkORyYCPwvSljNgKv7bavBC5Nkq7/8qr6JfDDJDu6+/vaPGsa6HWf2s73bv9pi7uWpOY2POogLnz248cy97DXZFYlORJ4Af9/4X++jgJum9Ke7PoGjule3XYPcNiQxwKQ5NwkE0kmdu7cuUClS5KGMeyZzEXAVcB1VbU1ye8BN89z7gzoqyHHDHNsv7NqM7AZoNfrDRwzm3H9H4AkLXdDhUxVfRT46JT2LcDz5zn3JHD0lPZa4PYZxkwmWQWsBnYNeawkacyGvfC/NsknktyZ5CdJPpZk7Tzn3gocl+SYJPvRv5C/ZdqYLfTfmwNwOvClqqqu/4zu1WfHAMcB35xnPZKkBTbsNZn30/+H/VH0r318qusbWXeN5Tz6T8PdCFxRVduTXJTkOd2w9wKHdRf2zwcu6I7dDlxB/0UCnwP+sqrun089kqSFl/6JwSyDkm9X1RNn61vqer1eTUxMjLsMSVpWkmyrqt4oxw57JnNXkhcl2be7vQj4r1EmlCStHMOGzNn0X778n8Ad9K+PvLhVUZKkvcOwHyvzH1X1nKpaU1VHVNVpwPMa1yZJWubm882Y5y9YFZKkvdJ8QmbQGyIlSfq1+YTMSO+elyStHHt8x3+SnzE4TAL8TpOKJEl7jT2GTFU9fLEKkSTtfebzdJkkSXtkyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDVjyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDVjyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKmZsYRMkkOTXJ3k5u7nITOM29SNuTnJpq7vgCSfSfL9JNuTXLy41UuShjWuM5kLgC9W1XHAF7v2b0hyKHAh8GTgeODCKWH01qp6HPCHwNOSnLo4ZUuS5mJcIbMRuKzbvgw4bcCYZwFXV9WuqrobuBo4pap+XlXXAFTVvcD1wNpFqFmSNEfjCplHVNUdAN3PIwaMOQq4bUp7suv7tSQHA8+mfzYkSVpiVrW64yRfAB45YNdrhr2LAX015f5XAR8G/q6qbtlDHecC5wKsW7duyKklSQuhWchU1Ykz7UvykyRHVtUdSY4E7hwwbBI4YUp7LXDtlPZm4OaqevssdWzuxtLr9WpPYyVJC2tcT5dtATZ125uATw4YcxVwcpJDugv+J3d9JHkDsBp4+SLUKkka0bhC5mLgpCQ3Ayd1bZL0krwHoKp2Aa8Htna3i6pqV5K19J9y2wBcn+TbSV4yjl9CkrRnqVo5zyD1er2amJgYdxmStKwk2VZVvVGO9R3/kqRmDBlJUjOGjCSpGUNGktSMISNJasaQkSQ1Y8hIkpoxZCRJzRgykqRmDBlJUjOGjCSpGUNGktSMISNJasaQkSQ1Y8hIkpoxZCRJzRgykqRmDBlJUjOGjCSpGUNGktSMISNJasaQkSQ1Y8hIkpoxZCRJzRgykqRmDBlJUjOGjCSpGUNGktSMISNJasaQkSQ1Y8hIkpoZS8gkOTTJ1Ulu7n4eMsO4Td2Ym5NsGrB/S5Lvtq9YkjSKcZ3JXAB8saqOA77YtX9DkkOBC4EnA8cDF04NoyTPA3YvTrmSpFGMK2Q2Apd125cBpw0Y8yzg6qraVVV3A1cDpwAkeRhwPvCGRahVkjSicYXMI6rqDoDu5xEDxhwF3DalPdn1Abwe+Fvg57NNlOTcJBNJJnbu3Dm/qiVJc7Kq1R0n+QLwyAG7XjPsXQzoqyRPBI6tqlckWT/bnVTVZmAzQK/XqyHnliQtgGYhU1UnzrQvyU+SHFlVdyQ5ErhzwLBJ4IQp7bXAtcBTgScluZV+/UckubaqTkCStKSM6+myLcCDrxbbBHxywJirgJOTHNJd8D8ZuKqq3l1Vj6qq9cDTgR8YMJK0NI0rZC4GTkpyM3BS1yZJL8l7AKpqF/1rL1u720VdnyRpmUjVyrlM0ev1amJiYtxlSNKykmRbVfVGOdZ3/EuSmjFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDVjyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDVjyEiSmjFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDWTqhp3DYsmyU7gR11zNXDPlN2ztQ8H7mpY3vT5FvK42cbMtH/Y/uW4dsMeszevXcvH3DDj5rJ2w/S5du3W7neras2eCp5RVa3IG7B5ju2JxaxnIY+bbcxM+4ftX45rN+wxe/PatXzMLfTaDdPn2i3NtVvJT5d9ao7t1kadb5jjZhsz0/5h+5fj2g17zN68di0fc8OMm8vaDdPn2i3BtVtRT5fNR5KJquqNu47lyLUbnWs3OtdudAu5div5TGauNo+7gGXMtRudazc61250C7Z2nslIkprxTEaS1IwhI0lqxpCRJDVjyCyAJOuSbEnyviQXjLue5STJM5L8fZL3JPnquOtZLpLsk+SNSd6RZNO461lOkpyQ5Cvd4+6Ecdez3CQ5MMm2JH82zPgVHzJdMNyZ5LvT+k9JclOSHUMEx2OAz1TV2cCGZsUuMQuxdlX1lap6KfBp4LKW9S4VC/SY2wgcBfwKmGxV61KzQGtXwG7gobh2c107gFcBVww970p/dVmSP6b/gPvHqvr9rm9f4AfASfQfhFuBM4F9gTdNu4uzgfuBK+k/eD9YVe9fnOrHayHWrqru7I67AnhJVf10kcofmwV6zJ0N3F1V/5Dkyqo6fbHqH6cFWru7quqBJI8A3lZVf75Y9Y/TAq3dE+h/5MxD6a/jp2ebd9VC/QLLVVV9Ocn6ad3HAzuq6haAJJcDG6vqTcBvnSImeSVwYXdfVwIrImQWYu26MeuAe1ZCwMCCPeYmgXu75v3tql1aFuox17kb2L9FnUvRAj3u/gQ4kP4zNr9I8i9V9cCe5l3xITODo4DbprQngSfvYfzngNcmeSFwa8O6loO5rh3AOayQYN6Dua7bx4F3JHkG8OWWhS0Dc1q7JM8DngUcDFzatrQlb05rV1WvAUhyFt0Z4WwTGDKDZUDfjM8rVtV3gRXxdMUQ5rR2AFV1YaNalpO5PuZ+Tj+cNfe1+zj9kNYIf68AVfWBYSdY8Rf+ZzAJHD2lvRa4fUy1LDeu3Whct9G5dqNrvnaGzGBbgeOSHJNkP+AMYMuYa1ouXLvRuG6jc+1G13ztVnzIJPkw8DXgsUkmk5xTVfcB5wFXATcCV1TV9nHWuRS5dqNx3Ubn2o1uXGu34l/CLElqZ8WfyUiS2jFkJEnNGDKSpGYMGUlSM4aMJKkZQ0aS1IwhoxUvye5Fnu89SRb1KyGSvDzJAYs5pwS+T0Yiye6qetgC3t+q7k1uiyZJ6P89D/zAwiS3Ar2qumsx65I8k5EGSLImyceSbO1uT+v6j0/y1STf6n4+tus/K8lHk3wK+Hz37YvXJrkyyfeT/HMXBHT9vW57d/rfcPmdJF/vvuOEJI/u2luTXDTobCvJ+iQ3JnkXcD1wdJJ3J5lIsj3J67pxLwMeBVyT5Jqu7+QkX0tyfVf3goWs9Buqypu3FX0Ddg/o+xDw9G57HXBjt30QsKrbPhH4WLd9Fv0PGzy0a58A3EP/Awf3of9xHg/e37X0zyqg/4m3z+623wL8Vbf9aeDMbvulM9S4HngAeMqUvgfn37eb5wld+1bg8G77cPpfD3Bg134V8Dfj/u/gbe+8+VH/0mAnAhu6kw+Ag5I8HFgNXJbkOPoB8ZApx1xdVbumtL9ZVZMASb5NPxT+ddo899IPFIBt9L+hEOCpwGnd9oeAt85Q54+q6utT2i9Ici79r/E4kv6XS90w7ZindP3Xdb/ffvRDUFpwhow02D7AU6vqF1M7k7wDuKaqntt9y+C1U3b/z7T7+OWU7fsZ/Pf2q6qqWcbsya/nTHIM8Ergj6rq7iQfoP81udOFfiCeOce5pDnzmow02OfpfzotAEme2G2uBn7cbZ/VcP6vA8/vts8Y8piD6IfOPd21nVOn7PsZ8PAp9/20JMcCJDkgyWPmX7L02wwZCQ7oPvr8wdv5wMuAXpIbknyP/nUR6F83eVOS6+hf92jl5cD5Sb5J/2mve2Y7oKq+A3wL2A68D7huyu7NwGeTXFNVO+kH5IeT3EA/dB63sOVLfb6EWVqCuve0/KKqKskZ9F8EsHHcdUlz5TUZaWl6EnBp97Ln/wbOHnM90kg8k5EkNeM1GUlSM4aMJKkZQ0aS1IwhI0lqxpCRJDVjyEiSmvk/h//+LvH1rEkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(logs, losses)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
