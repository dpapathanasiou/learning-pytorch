{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Another machine translation sketch, based on the [torchtext tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html).\n",
    "\n",
    "For comparison with the [seq2seq version](MT-pytorch-seq2seq-tutorial.ipynb), using the same Spanish-English dataset from [Anki](http://www.manythings.org/anki/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ ! -f ./data/spa-eng.zip ]; then wget -P ./data https://www.manythings.org/anki/spa-eng.zip; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ ! -f ./data/spa.txt ]; then unzip -n -d ./data ./data/spa-eng.zip; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/cb/28/91f26bd088ce8e22169032100d4260614fc3da435025ff389ef1d396a433/pip-20.2.4-py2.py3-none-any.whl (1.5MB)\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 19.2.1\n",
      "    Uninstalling pip-19.2.1:\n",
      "      Successfully uninstalled pip-19.2.1\n",
      "Successfully installed pip-20.2.4\n",
      "Collecting spacy\n",
      "  Downloading spacy-2.3.3-cp36-cp36m-manylinux2014_x86_64.whl (10.4 MB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (41.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.0)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.3-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.4-cp36-cp36m-manylinux2014_x86_64.whl (284 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.25.0)\n",
      "Collecting blis<0.8.0,>=0.4.0; python_version >= \"3.6\"\n",
      "  Downloading blis-0.7.3-cp36-cp36m-manylinux2014_x86_64.whl (9.8 MB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.4-cp36-cp36m-manylinux2014_x86_64.whl (293 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.53.0)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.4-cp36-cp36m-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.4-cp36-cp36m-manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Collecting importlib-metadata>=0.20; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-3.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: murmurhash, cymem, preshed, blis, zipp, importlib-metadata, catalogue, plac, srsly, wasabi, thinc, spacy\n",
      "Successfully installed blis-0.7.3 catalogue-1.0.0 cymem-2.0.4 importlib-metadata-3.1.0 murmurhash-1.0.4 plac-1.1.3 preshed-3.0.4 spacy-2.3.3 srsly-1.0.4 thinc-7.4.3 wasabi-0.8.0 zipp-3.4.0\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.53.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (41.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=e23f2cab735ae62f3df76772e3e827cc9bcfc5cda8c072bd0ba2ac6b30a402dc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8gi74f4k/wheels/10/6f/a6/ddd8204ceecdedddea923f8514e13afb0c1f0f556d2c9c3da0\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting es_core_news_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.3.1/es_core_news_sm-2.3.1.tar.gz (16.2 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from es_core_news_sm==2.3.1) (2.3.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (2.25.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (41.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (1.17.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (4.53.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (7.4.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (0.7.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->es_core_news_sm==2.3.1) (3.4.0)\n",
      "Building wheels for collected packages: es-core-news-sm\n",
      "  Building wheel for es-core-news-sm (setup.py): started\n",
      "  Building wheel for es-core-news-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.3.1-py3-none-any.whl size=16216999 sha256=5d3ae233cbef077a9419b9148da7ee220492a2cbc4f4eabbcc00d3045aaea331\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ed50iq10/wheels/ef/67/82/0b6f50c9f4a2ad0e01633af17ece1d0bd8244f883385b1d9fa\n",
      "Successfully built es-core-news-sm\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
      "You can now load the model via spacy.load('es')\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade pip\n",
    "pip3 install spacy\n",
    "python3 -m spacy download en\n",
    "python3 -m spacy download es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [torchtext.data](https://torchtext.readthedocs.io/en/latest/data.html) needs a parser, for which [spaCy](https://spacy.io/) is useful:\n",
    "\n",
    "```sh\n",
    "$ pip install spacy\n",
    "\n",
    "$ python -m spacy download en\n",
    "...\n",
    "✔ Download and installation successful\n",
    "You can now load the model via spacy.load('en_core_web_sm')\n",
    "✔ Linking successful\n",
    "/home/denis/anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
    "/home/denis/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
    "You can now load the model via spacy.load('en')\n",
    "\n",
    "$ python -m spacy download es\n",
    "Collecting es_core_news_sm==2.2.5\n",
    "...\n",
    "Successfully built es-core-news-sm\n",
    "Installing collected packages: es-core-news-sm\n",
    "Successfully installed es-core-news-sm-2.2.5\n",
    "✔ Download and installation successful\n",
    "You can now load the model via spacy.load('es_core_news_sm')\n",
    "✔ Linking successful\n",
    "/home/denis/anaconda3/lib/python3.7/site-packages/es_core_news_sm -->\n",
    "/home/denis/anaconda3/lib/python3.7/site-packages/spacy/data/es\n",
    "You can now load the model via spacy.load('es')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "format of the samples is:\n",
    "[english] tab [spanish]\n",
    "''' \n",
    "\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True,\n",
    "            sequential=True)\n",
    "\n",
    "TGT = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"es\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True,\n",
    "            sequential=True,\n",
    "            is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = TabularDataset(path='./data/spa.txt',\n",
    "                         format='tsv', \n",
    "                         fields = [(\"src\", SRC), (\"tgt\", TGT)],\n",
    "                         skip_header=True)\n",
    "\n",
    "(training, testing, validating) = samples.split(split_ratio=[0.6,0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75267, 25089, 25089)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(training),len(testing),len(validating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(training, min_freq = 2)\n",
    "TGT.build_vocab(training, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7281"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SRC.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12015"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TGT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import random\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (training, testing, validating),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device,\n",
    "    sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tuple[Tensor]:\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim: int,\n",
    "                 emb_dim: int,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: int,\n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "        return weighted_encoder_rep\n",
    "\n",
    "    def forward(self,\n",
    "                input: Tensor,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_encoder_rep,\n",
    "                                     embedded), dim = 1))\n",
    "        return output, decoder_hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = trg[0,:]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an instance of the model with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,411,671 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TGT.vocab)\n",
    "\n",
    "ENC_EMB_DIM = 32\n",
    "DEC_EMB_DIM = 32\n",
    "ENC_HID_DIM = 64\n",
    "DEC_HID_DIM = 64\n",
    "ATTN_DIM = 8\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TGT.vocab.stoi['<pad>'])\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: BucketIterator,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        tgt = batch.tgt\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        tgt = tgt[1:].view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model: nn.Module,\n",
    "             iterator: BucketIterator,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            tgt = batch.tgt\n",
    "            output = model(src, tgt, 0) #turn off teacher forcing\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            tgt = tgt[1:].view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 34s\n",
      "\tTrain Loss: 5.579 | Train PPL: 264.787\n",
      "\t Val. Loss: 5.251 |  Val. PPL: 190.815\n",
      "Epoch: 02 | Time: 1m 36s\n",
      "\tTrain Loss: 4.891 | Train PPL: 133.042\n",
      "\t Val. Loss: 5.003 |  Val. PPL: 148.905\n",
      "Epoch: 03 | Time: 1m 34s\n",
      "\tTrain Loss: 4.630 | Train PPL: 102.564\n",
      "\t Val. Loss: 4.827 |  Val. PPL: 124.793\n",
      "Epoch: 04 | Time: 1m 34s\n",
      "\tTrain Loss: 4.400 | Train PPL:  81.433\n",
      "\t Val. Loss: 4.636 |  Val. PPL: 103.174\n",
      "Epoch: 05 | Time: 1m 35s\n",
      "\tTrain Loss: 4.174 | Train PPL:  64.951\n",
      "\t Val. Loss: 4.409 |  Val. PPL:  82.158\n",
      "Epoch: 06 | Time: 1m 35s\n",
      "\tTrain Loss: 3.930 | Train PPL:  50.891\n",
      "\t Val. Loss: 4.145 |  Val. PPL:  63.126\n",
      "Epoch: 07 | Time: 1m 34s\n",
      "\tTrain Loss: 3.667 | Train PPL:  39.143\n",
      "\t Val. Loss: 3.953 |  Val. PPL:  52.089\n",
      "Epoch: 08 | Time: 1m 34s\n",
      "\tTrain Loss: 3.444 | Train PPL:  31.319\n",
      "\t Val. Loss: 3.751 |  Val. PPL:  42.580\n",
      "Epoch: 09 | Time: 1m 35s\n",
      "\tTrain Loss: 3.245 | Train PPL:  25.660\n",
      "\t Val. Loss: 3.581 |  Val. PPL:  35.915\n",
      "Epoch: 10 | Time: 1m 35s\n",
      "\tTrain Loss: 3.085 | Train PPL:  21.869\n",
      "\t Val. Loss: 3.453 |  Val. PPL:  31.598\n",
      "| Test Loss: 3.458 | Test PPL:  31.769 |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "    \n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Use the trained model to come up with translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, tgt_field, model, device, max_len, src_lang):\n",
    "    model.eval()\n",
    "        \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load(src_lang)\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor) #, src_len)\n",
    "\n",
    "    tgt_indexes = [tgt_field.vocab.stoi[tgt_field.init_token]]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        tgt_tensor = torch.LongTensor([tgt_indexes[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(tgt_tensor, hidden, encoder_outputs)\n",
    "\n",
    "        pred_token = output.argmax(1).item()\n",
    "        tgt_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == tgt_field.vocab.stoi[tgt_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    tgt_tokens = [tgt_field.vocab.itos[i] for i in tgt_indexes] \n",
    "    return tgt_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['i', 'will', 'give', 'you', 'a', 'bicycle', 'for', 'your', 'birthday', '.']\n",
      "tgt = ['te', 'regalaré', 'una', 'bicicleta', 'para', 'tu', 'cumpleaños', '.']\n",
      "predicted = ['te', 'gustaría', 'un', 'coche', 'tu', 'tu', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "How to inspect the TabularDataset,\n",
    "and request a translation\n",
    "'''\n",
    "\n",
    "example_item = training.examples[12] # 12 as a random index\n",
    "\n",
    "src = vars(example_item)['src']\n",
    "tgt = vars(example_item)['tgt']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'tgt = {tgt}')\n",
    "\n",
    "# use the src value as the input\n",
    "translation = translate_sentence(src, SRC, TGT, model, device, 100, 'en')\n",
    "\n",
    "print(f'predicted = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_item(item):\n",
    "    src = vars(item)['src']\n",
    "    tgt = vars(item)['tgt']\n",
    "    print('>', f'{src}')\n",
    "    print('=', f'{tgt}')\n",
    "    translation = translate_sentence(src, SRC, TGT, model, device, 100, 'en')\n",
    "    print('<', f'{translation}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['your', 'sister', 'is', 'a', 'good', 'pianist', ',', 'is', \"n't\", 'she', '?']\n",
      "= ['tu', 'hermana', 'es', 'una', 'buena', 'pianista', ',', '¿', 'verdad', '?']\n",
      "< ['tu', 'hijo', 'es', 'un', 'buen', 'hombre', ',', '¿', 'no', 'es', '?', '<eos>']\n",
      "\n",
      "> ['he', 'carried', 'a', 'bundle', 'of', 'clothes', '.']\n",
      "= ['llevaba', 'un', 'lío', 'de', 'ropa', '.']\n",
      "< ['él', 'se', 'un', 'un', 'de', 'de', '.', '<eos>']\n",
      "\n",
      "> ['i', 'do', \"n't\", 'think', 'people', 'should', 'make', 'a', 'mountain', 'of', 'a', 'mole', 'hill', '.']\n",
      "= ['no', 'creo', 'que', 'la', 'gente', 'deba', 'hacer', 'una', 'montaña', 'de', 'un', 'grano', 'de', 'arena', '.']\n",
      "< ['no', 'me', 'gusta', 'la', 'gente', 'que', '<unk>', 'un', 'accidente', 'de', 'un', '<unk>', '.', '<eos>']\n",
      "\n",
      "> ['i', 'thought', 'it', 'was', 'a', 'mistake', 'to', 'tell', 'tom', 'the', 'secret', '.']\n",
      "= ['pensé', 'que', 'era', 'un', 'error', 'decirle', 'a', 'tom', 'el', 'secreto', '.']\n",
      "< ['pensé', 'que', 'era', 'un', 'un', 'que', 'tom', 'que', 'la', 'la', '.', '<eos>']\n",
      "\n",
      "> ['i', 'am', 'paid', '10', 'dollars', 'an', 'hour', '.']\n",
      "= ['me', 'pagan', '10', 'dólares', 'la', 'hora', '.']\n",
      "< ['me', 'levanté', 'a', 'horas', '.', '<eos>']\n",
      "\n",
      "> ['i', \"'d\", 'like', 'to', 'hear', 'your', 'advice', '.']\n",
      "= ['me', 'gustaría', 'oír', 'tu', 'consejo', '.']\n",
      "< ['me', 'gustaría', 'tu', 'tu', '.', '.', '<eos>']\n",
      "\n",
      "> ['do', \"n't\", 'give', 'up', '!']\n",
      "= ['no', 'tires', 'la', 'toalla', '.']\n",
      "< ['no', 'te', 'no', 'no', '<unk>', '?', '<eos>']\n",
      "\n",
      "> ['when', 'do', 'you', 'mean', 'to', 'start', '?']\n",
      "= ['¿', 'cuándo', 'pretendes', 'empezar', '?']\n",
      "< ['¿', 'cuándo', 'quieres', 'que', '?', '?', '<eos>']\n",
      "\n",
      "> ['he', 'was', 'sentenced', 'to', 'community', 'service', '.']\n",
      "= ['él', 'fue', 'condenado', 'a', 'realizar', 'servicios', 'comunitarios', '.']\n",
      "< ['él', 'fue', '<unk>', 'a', 'los', '<unk>', '.', '<eos>']\n",
      "\n",
      "> ['we', 'saw', 'a', 'dim', 'light', 'in', 'the', 'distance', '.']\n",
      "= ['vimos', 'una', 'débil', 'luz', 'en', 'la', 'distancia', '.']\n",
      "< ['nos', 'un', 'un', '<unk>', 'en', 'en', 'la', '.', '.', '<eos>']\n",
      "\n",
      "> ['i', \"'m\", 'sick', 'of', 'hamburgers', '.']\n",
      "= ['estoy', 'harto', 'de', 'hamburguesas', '.']\n",
      "< ['estoy', 'harto', 'de', '.', '.', '<eos>']\n",
      "\n",
      "> ['i', 'want', 'to', 'eat', 'out', 'tonight', '.']\n",
      "= ['esta', 'noche', 'quiero', 'salir', 'a', 'comer', '.']\n",
      "< ['quiero', 'quedarme', 'en', 'boston', '.', '<eos>']\n",
      "\n",
      "> ['keep', 'reading', '.']\n",
      "= ['sigue', 'leyendo', '.']\n",
      "< ['<unk>', 'de', '.', '.', '<eos>']\n",
      "\n",
      "> ['i', 'know', 'i', 'should', \"n't\", 'care', 'what', 'tom', 'thinks', '.']\n",
      "= ['sé', 'que', 'no', 'debería', 'importarme', 'lo', 'que', 'tom', 'piense', '.']\n",
      "< ['sé', 'que', 'no', 'no', 'me', 'dijiste', 'que', 'tom', 'tom', '.', '<eos>']\n",
      "\n",
      "> ['i', 'assumed', 'that', 'tom', 'and', 'mary', 'were', 'husband', 'and', 'wife', '.']\n",
      "= ['asumí', 'que', 'tom', 'y', 'mary', 'eran', 'marido', 'y', 'mujer', '.']\n",
      "< ['asumí', 'que', 'tom', 'y', 'mary', 'mary', 'mary', 'y', 'mary', 'y', 'mary', '.', '<eos>']\n",
      "\n",
      "> ['this', 'is', 'the', 'end', 'of', 'my', 'story', '.']\n",
      "= ['este', 'es', 'el', 'final', 'de', 'mi', 'historia', '.']\n",
      "< ['este', 'es', 'el', 'de', 'mi', 'mi', '.', '.', '<eos>']\n",
      "\n",
      "> ['i', 'just', 'wanted', 'you', 'to', 'know', 'that', '.']\n",
      "= ['sólo', 'deseaba', 'que', 'lo', 'conocieras', '.']\n",
      "< ['solo', 'quiero', 'saber', 'eso', '.', '<eos>']\n",
      "\n",
      "> ['who', 'can', 'i', 'talk', 'to', '?']\n",
      "= ['¿', 'con', 'quién', 'puedo', 'hablar', '?']\n",
      "< ['¿', 'quién', 'puedo', 'hablar', 'con', 'tom', '?', '<eos>']\n",
      "\n",
      "> ['tom', 'just', 'left', '.']\n",
      "= ['tom', 'se', 'acaba', 'de', 'ir', '.']\n",
      "< ['tom', 'se', 'a', '.', '.', '<eos>']\n",
      "\n",
      "> ['tom', 'and', 'mary', 'are', 'holding', 'hands', '.']\n",
      "= ['tom', 'y', 'mary', 'van', 'de', 'la', 'mano', '.']\n",
      "< ['tom', 'y', 'mary', 'son', 'en', 'los', '.', '.', '<eos>']\n",
      "\n",
      "> ['could', 'you', 'please', 'tell', 'me', 'again', 'where', 'you', 'put', 'the', 'key', '?']\n",
      "= ['¿', 'podrías', 'decirme', 'nuevamente', 'dónde', 'pusiste', 'la', 'llave', ',', 'por', 'favor', '?']\n",
      "< ['¿', 'podrías', 'repetirme', 'favor', ',', '¿', 'qué', '?', 'la', 'mesa', '?', '<eos>']\n",
      "\n",
      "> ['can', 'you', 'tie', 'a', 'bow', '?']\n",
      "= ['¿', 'puede', 'usted', 'hacer', 'un', 'lazo', '?']\n",
      "< ['¿', 'puedes', 'usar', 'un', 'mapa', '?', '<eos>']\n",
      "\n",
      "> ['driving', 'through', 'that', 'snowstorm', 'was', 'a', 'nightmare', '.']\n",
      "= ['conducir', 'en', 'medio', 'de', 'una', 'tormenta', 'fue', 'una', 'pesadilla', '.']\n",
      "< ['<unk>', '<unk>', 'es', 'un', '<unk>', 'un', 'un', '.', '.', '<eos>']\n",
      "\n",
      "> ['he', 'can', 'swim', 'faster', 'than', 'any', 'other', 'boy', 'in', 'his', 'class', '.']\n",
      "= ['él', 'puede', 'nadar', 'más', 'rápido', 'que', 'cualquier', 'otro', 'chico', 'de', 'su', 'clase', '.']\n",
      "< ['él', 'puede', 'nadar', 'más', 'más', 'que', 'que', 'su', 'en', 'su', 'casa', '.', '<eos>']\n",
      "\n",
      "> ['did', 'you', 'need', 'something', '?']\n",
      "= ['¿', 'necesitabas', 'algo', '?']\n",
      "< ['¿', 'necesitas', 'algo', '?', '<eos>']\n",
      "\n",
      "> ['tom', 'and', 'mary', 'have', 'three', 'cats', 'and', 'two', 'dogs', '.']\n",
      "= ['tom', 'y', 'mary', 'tienen', 'tres', 'gatos', 'y', 'dos', 'perros', '.']\n",
      "< ['tom', 'y', 'mary', 'mary', 'tres', 'hijos', 'y', 'dos', 'gatos', '.', '<eos>']\n",
      "\n",
      "> ['tom', 'and', 'i', 'swim', 'together', 'three', 'times', 'a', 'week', '.']\n",
      "= ['tom', 'y', 'yo', 'vamos', 'juntos', 'a', 'nadar', 'tres', 'veces', 'a', 'la', 'semana', '.']\n",
      "< ['tom', 'y', 'tom', 'tres', 'tres', 'tres', 'veces', 'y', 'tres', 'semana', '.', '<eos>']\n",
      "\n",
      "> ['i', 'like', 'a', 'pen', 'with', 'a', 'fine', 'point', '.']\n",
      "= ['me', 'gusta', 'una', 'pluma', 'fina', '.']\n",
      "< ['me', 'gusta', 'un', 'un', 'un', '.', '.', '<eos>']\n",
      "\n",
      "> ['i', 'want', 'what', \"'s\", 'best', 'for', 'tom', '.']\n",
      "= ['quiero', 'lo', 'mejor', 'para', 'tom', '.']\n",
      "< ['quiero', 'saber', 'qué', 'mejor', 'que', 'tom', 'tom', '.', '<eos>']\n",
      "\n",
      "> ['tom', 'gave', 'me', 'some', 'advice', '.']\n",
      "= ['tom', 'me', 'dio', 'algunos', 'consejos', '.']\n",
      "< ['tom', 'me', 'dio', 'un', 'poco', 'de', '.', '.', '<eos>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample some random entries\n",
    "\n",
    "for i in range(30):\n",
    "    evaluate_item(random.choice(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
