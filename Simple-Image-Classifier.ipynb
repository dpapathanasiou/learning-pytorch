{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Image Classifier\n",
    "\n",
    "Implementing Chapter 2 of [Programming PyTorch for Deep Learning](http://shop.oreilly.com/product/0636920216032.do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "This is a simple three layer network: an input and hidden layer, with a two-node output layer.\n",
    "\n",
    "It is *fully-connected* in that each node in each layer affects every node the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(12288, 84)\n",
    "        self.hidden_layer = nn.Linear(84, 50)\n",
    "        self.output_layer = nn.Linear(50, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 12288)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = F.relu(self.hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(simple.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, training_loader, validation_loader, epochs, device=\"cpu\"):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        model.train()\n",
    "        for (inputs, targets) in training_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item()\n",
    "        training_loss /= len(training_loader)\n",
    "        \n",
    "        number_correct = 0\n",
    "        number_examples = 0\n",
    "        model.eval()\n",
    "        for (inputs, targets) in validation_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            validation_loss += loss.data.item()\n",
    "            correct = torch.eq(torch.max(F.softmax(outputs), dim=1)[1], targets).view(-1)\n",
    "            number_correct += torch.sum(correct).item()\n",
    "            number_examples += correct.shape[0]\n",
    "        validation_loss /= len(validation_loader)\n",
    "        \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(\n",
    "            epoch, \n",
    "            training_loss,\n",
    "            validation_loss, \n",
    "            number_correct / number_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Using the [technique from lesson 2](https://github.com/dpapathanasiou/course-v3/blob/master/nbs/dl1/lesson2-download.ipynb) of the [Fast.ai course](https://course.fast.ai/), get a list of image urls with some in-browser javascript:\n",
    "\n",
    "```javascript\n",
    "urls=Array.from(document.querySelectorAll('.rg_i')).map(el=> el.hasAttribute('data-src')?el.getAttribute('data-src'):el.getAttribute('data-iurl'));window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\\n')));\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "def fetch_images(image_url_list, target):\n",
    "    data_file = Path(image_url_list)\n",
    "    for i, url in enumerate(data_file.read_text().splitlines()):\n",
    "        image_file = Path(target) / str(i) # TODO: determine file extension, since ImageFolders needs it\n",
    "        urllib.request.urlretrieve(url, image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `cat` versus `fish`, this notebook will attempt [aikido](https://en.wikipedia.org/wiki/Aikido) verus [judo](https://en.wikipedia.org/wiki/Judo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_images(\"./data/aikido_train.csv\", \"./data/train/aikido\")\n",
    "fetch_images(\"./data/aikido_validate.csv\", \"./data/validate/aikido\")\n",
    "fetch_images(\"./data/aikido_test.csv\", \"./data/test/aikido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_images(\"./data/judo_train.csv\", \"./data/train/judo\")\n",
    "fetch_images(\"./data/judo_validate.csv\", \"./data/validate/judo\")\n",
    "fetch_images(\"./data/judo_test.csv\", \"./data/test/judo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose(\n",
    "    [transforms.Resize((64,64)), \n",
    "     transforms.ToTensor(),    \n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],                    \n",
    "                          std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data   = torchvision.datasets.ImageFolder(root=\"./data/train\",    transform=image_transform)\n",
    "validation_data = torchvision.datasets.ImageFolder(root=\"./data/validate\", transform=image_transform)\n",
    "test_data       = torchvision.datasets.ImageFolder(root=\"./data/test\",     transform=image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "training_loader   = DataLoader(training_data, batch_size=batch_size)\n",
    "validation_loader = DataLoader(validation_data, batch_size=batch_size)\n",
    "test_loader       = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.81, Validation Loss: 0.45, accuracy = 0.82\n",
      "Epoch: 1, Training Loss: 0.35, Validation Loss: 0.39, accuracy = 0.82\n",
      "Epoch: 2, Training Loss: 0.29, Validation Loss: 0.35, accuracy = 0.82\n",
      "Epoch: 3, Training Loss: 0.21, Validation Loss: 0.33, accuracy = 0.82\n",
      "Epoch: 4, Training Loss: 0.14, Validation Loss: 0.33, accuracy = 0.82\n",
      "Epoch: 5, Training Loss: 0.11, Validation Loss: 0.32, accuracy = 0.82\n",
      "Epoch: 6, Training Loss: 0.08, Validation Loss: 0.32, accuracy = 0.82\n",
      "Epoch: 7, Training Loss: 0.06, Validation Loss: 0.32, accuracy = 0.82\n",
      "Epoch: 8, Training Loss: 0.05, Validation Loss: 0.32, accuracy = 0.82\n",
      "Epoch: 9, Training Loss: 0.04, Validation Loss: 0.32, accuracy = 0.82\n",
      "Epoch: 10, Training Loss: 0.03, Validation Loss: 0.31, accuracy = 0.82\n",
      "Epoch: 11, Training Loss: 0.03, Validation Loss: 0.31, accuracy = 0.82\n",
      "Epoch: 12, Training Loss: 0.03, Validation Loss: 0.31, accuracy = 0.82\n",
      "Epoch: 13, Training Loss: 0.02, Validation Loss: 0.31, accuracy = 0.80\n",
      "Epoch: 14, Training Loss: 0.02, Validation Loss: 0.31, accuracy = 0.80\n",
      "Epoch: 15, Training Loss: 0.02, Validation Loss: 0.31, accuracy = 0.80\n",
      "Epoch: 16, Training Loss: 0.02, Validation Loss: 0.31, accuracy = 0.80\n",
      "Epoch: 17, Training Loss: 0.01, Validation Loss: 0.31, accuracy = 0.80\n",
      "Epoch: 18, Training Loss: 0.01, Validation Loss: 0.31, accuracy = 0.80\n",
      "Epoch: 19, Training Loss: 0.01, Validation Loss: 0.31, accuracy = 0.80\n"
     ]
    }
   ],
   "source": [
    "train(simple, optimizer, torch.nn.CrossEntropyLoss(), training_loader, validation_loader, 20, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "How does the model do against the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device=\"cpu\"):\n",
    "    i = 0\n",
    "    for (inputs, targets) in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        correct = torch.eq(torch.max(F.softmax(outputs), dim=1)[1], targets)\n",
    "        right = len(list(filter(lambda x: x.item(), list(correct))))\n",
    "        wrong = batch_size - right\n",
    "        print('Batch {}: {} right vs {} wrong (pct correct {:.2f})'.format(i, right, wrong, (right / batch_size)))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: 7 right vs 3 wrong (pct correct 0.70)\n",
      "Batch 1: 8 right vs 2 wrong (pct correct 0.80)\n",
      "Batch 2: 6 right vs 4 wrong (pct correct 0.60)\n",
      "Batch 3: 8 right vs 2 wrong (pct correct 0.80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "test(simple, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
